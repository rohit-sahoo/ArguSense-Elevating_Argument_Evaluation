{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":317.449491,"end_time":"2021-12-27T16:26:13.642034","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-12-27T16:20:56.192543","version":"2.3.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Configuration\nThis code snippet is used to configure and run a Python script for utilizing the Longformer model, a variant of the Transformer architecture designed to handle long-range dependencies in text data. It involves setting up GPU usage, versioning, loading tokens, and loading or downloading a pre-trained Longformer model.\n\nGPU Configuration: The code first sets the environment variable CUDA_VISIBLE_DEVICES to specify which GPU(s) to use for computation. In this case, it's set to use GPU 0. This is useful for systems with multiple GPUs.\n\nVersion and Paths: The code assigns a version number VER for saving and loading model weights. It specifies paths for loading tokens (LOAD_TOKENS_FROM) and loading a pre-trained model (LOAD_MODEL_FROM). If these paths are None, the notebook computes tokens and trains a new model. If they are provided, the notebook loads tokens and/or a pre-trained model from the specified paths.\n\nModel Download Path: The code checks if DOWNLOADED_MODEL_PATH is provided. If it's None, the notebook assumes that the Hugging Face configuration, tokenizer, and model will be downloaded from the internet ('model' is used as a placeholder). If a path is provided, the notebook uses the specified path to load these components.\n\nModel Name: The variable MODEL_NAME is set to 'allenai/longformer-base-4096', indicating the specific variant of the Longformer model to be used.","metadata":{"papermill":{"duration":0.032195,"end_time":"2021-12-27T16:21:04.125756","exception":false,"start_time":"2021-12-27T16:21:04.093561","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING/LOADING MODEL WEIGHTS\n# THIS SHOULD MATCH THE MODEL IN LOAD_MODEL_FROM\nVER=14 \n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/tf-longformer-v12'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '../input/tflongformerv14'\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/tf-longformer-v12'\n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'allenai/longformer-base-4096'","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:04.231587Z","iopub.status.busy":"2021-12-27T16:21:04.230699Z","iopub.status.idle":"2021-12-27T16:21:04.239378Z","shell.execute_reply":"2021-12-27T16:21:04.238829Z","shell.execute_reply.started":"2021-12-27T15:41:32.300809Z"},"papermill":{"duration":0.069069,"end_time":"2021-12-27T16:21:04.239511","exception":false,"start_time":"2021-12-27T16:21:04.170442","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('model')\n\n    config = AutoConfig.from_pretrained(MODEL_NAME) \n    config.save_pretrained('model')\n\n    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n    backbone.save_pretrained('model')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:04.386683Z","iopub.status.busy":"2021-12-27T16:21:04.385886Z","iopub.status.idle":"2021-12-27T16:21:04.390852Z","shell.execute_reply":"2021-12-27T16:21:04.391433Z","shell.execute_reply.started":"2021-12-27T15:41:32.331814Z"},"papermill":{"duration":0.061587,"end_time":"2021-12-27T16:21:04.391641","exception":false,"start_time":"2021-12-27T16:21:04.330054","status":"completed"},"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The above saves the files\n* TOKENIZER FILES - merges.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json, vocab.json\n* CONFIG FILE - config.json\n* MODEL WEIGHT FILE - tf_model.h5","metadata":{"papermill":{"duration":0.027572,"end_time":"2021-12-27T16:21:04.461057","exception":false,"start_time":"2021-12-27T16:21:04.433485","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{"papermill":{"duration":0.027788,"end_time":"2021-12-27T16:21:04.518347","exception":false,"start_time":"2021-12-27T16:21:04.490559","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom transformers import *\nprint('TF version',tf.__version__)","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:04.579232Z","iopub.status.busy":"2021-12-27T16:21:04.578555Z","iopub.status.idle":"2021-12-27T16:21:14.902562Z","shell.execute_reply":"2021-12-27T16:21:14.903161Z","shell.execute_reply.started":"2021-12-27T15:41:32.343257Z"},"papermill":{"duration":10.356672,"end_time":"2021-12-27T16:21:14.903361","exception":false,"start_time":"2021-12-27T16:21:04.546689","status":"completed"},"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"TF version 2.6.2\n"}]},{"cell_type":"code","source":"# USE MULTIPLE GPUS\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:14.964043Z","iopub.status.busy":"2021-12-27T16:21:14.963418Z","iopub.status.idle":"2021-12-27T16:21:14.972488Z","shell.execute_reply":"2021-12-27T16:21:14.973397Z","shell.execute_reply.started":"2021-12-27T15:41:42.630383Z"},"papermill":{"duration":0.041565,"end_time":"2021-12-27T16:21:14.973567","exception":false,"start_time":"2021-12-27T16:21:14.932002","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"single strategy\n"}]},{"cell_type":"code","source":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:15.033294Z","iopub.status.busy":"2021-12-27T16:21:15.032692Z","iopub.status.idle":"2021-12-27T16:21:15.035394Z","shell.execute_reply":"2021-12-27T16:21:15.035987Z","shell.execute_reply.started":"2021-12-27T15:41:42.646839Z"},"papermill":{"duration":0.034577,"end_time":"2021-12-27T16:21:15.036149","exception":false,"start_time":"2021-12-27T16:21:15.001572","status":"completed"},"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"Mixed precision enabled\n"}]},{"cell_type":"markdown","source":"# Load the dataset","metadata":{"papermill":{"duration":0.027416,"end_time":"2021-12-27T16:21:15.092153","exception":false,"start_time":"2021-12-27T16:21:15.064737","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\nprint( train.shape )\ntrain.head()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:15.152064Z","iopub.status.busy":"2021-12-27T16:21:15.151552Z","iopub.status.idle":"2021-12-27T16:21:17.127873Z","shell.execute_reply":"2021-12-27T16:21:17.129552Z","shell.execute_reply.started":"2021-12-27T15:41:42.656512Z"},"papermill":{"duration":2.01025,"end_time":"2021-12-27T16:21:17.130175","exception":false,"start_time":"2021-12-27T16:21:15.119925","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"(144293, 8)\n"},{"execution_count":6,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>discourse_id</th>\n","      <th>discourse_start</th>\n","      <th>discourse_end</th>\n","      <th>discourse_text</th>\n","      <th>discourse_type</th>\n","      <th>discourse_type_num</th>\n","      <th>predictionstring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>8.0</td>\n","      <td>229.0</td>\n","      <td>Modern humans today are always on their phone....</td>\n","      <td>Lead</td>\n","      <td>Lead 1</td>\n","      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>230.0</td>\n","      <td>312.0</td>\n","      <td>They are some really bad consequences when stu...</td>\n","      <td>Position</td>\n","      <td>Position 1</td>\n","      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>313.0</td>\n","      <td>401.0</td>\n","      <td>Some certain areas in the United States ban ph...</td>\n","      <td>Evidence</td>\n","      <td>Evidence 1</td>\n","      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>402.0</td>\n","      <td>758.0</td>\n","      <td>When people have phones, they know about certa...</td>\n","      <td>Evidence</td>\n","      <td>Evidence 2</td>\n","      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>759.0</td>\n","      <td>886.0</td>\n","      <td>Driving is one of the way how to get around. P...</td>\n","      <td>Claim</td>\n","      <td>Claim 1</td>\n","      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  discourse_id  discourse_start  discourse_end  \\\n","0  423A1CA112E2  1.622628e+12              8.0          229.0   \n","1  423A1CA112E2  1.622628e+12            230.0          312.0   \n","2  423A1CA112E2  1.622628e+12            313.0          401.0   \n","3  423A1CA112E2  1.622628e+12            402.0          758.0   \n","4  423A1CA112E2  1.622628e+12            759.0          886.0   \n","\n","                                      discourse_text discourse_type  \\\n","0  Modern humans today are always on their phone....           Lead   \n","1  They are some really bad consequences when stu...       Position   \n","2  Some certain areas in the United States ban ph...       Evidence   \n","3  When people have phones, they know about certa...       Evidence   \n","4  Driving is one of the way how to get around. P...          Claim   \n","\n","  discourse_type_num                                   predictionstring  \n","0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n","1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n","2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n","3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n","4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "]},"metadata":{}}]},{"cell_type":"code","source":"print('The train labels are:')\ntrain.discourse_type.unique()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:17.267324Z","iopub.status.busy":"2021-12-27T16:21:17.261490Z","iopub.status.idle":"2021-12-27T16:21:17.291145Z","shell.execute_reply":"2021-12-27T16:21:17.292305Z","shell.execute_reply.started":"2021-12-27T15:41:44.934148Z"},"papermill":{"duration":0.092341,"end_time":"2021-12-27T16:21:17.292492","exception":false,"start_time":"2021-12-27T16:21:17.200151","status":"completed"},"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"The train labels are:\n"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":["array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n","       'Counterclaim', 'Rebuttal'], dtype=object)"]},"metadata":{}}]},{"cell_type":"code","source":"IDS = train.id.unique()\nprint('There are',len(IDS),'train texts.')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:17.410751Z","iopub.status.busy":"2021-12-27T16:21:17.409903Z","iopub.status.idle":"2021-12-27T16:21:17.432675Z","shell.execute_reply":"2021-12-27T16:21:17.433913Z","shell.execute_reply.started":"2021-12-27T15:41:44.958304Z"},"papermill":{"duration":0.087429,"end_time":"2021-12-27T16:21:17.434112","exception":false,"start_time":"2021-12-27T16:21:17.346683","status":"completed"},"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"There are 15594 train texts.\n"}]},{"cell_type":"markdown","source":"# Tokenize Train\nThe following code converts Kaggle's train dataset into a NER token array that we can use to train a NER transformer. I have made it very clear which targets belong to which class. This allows us to very easily convert this code to `Question Answer formulation` if we want. Just change the 14 NER arrays to be 14 arrays of `start position` and `end position` for each of the 7 classes.","metadata":{"papermill":{"duration":0.092016,"end_time":"2021-12-27T16:21:17.626952","exception":false,"start_time":"2021-12-27T16:21:17.534936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MAX_LEN = 1024\n\n# THE TOKENS AND ATTENTION ARRAYS\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\ntrain_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\ntrain_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n\n# THE 14 CLASSES FOR NER\nlead_b = np.zeros((len(IDS),MAX_LEN))\nlead_i = np.zeros((len(IDS),MAX_LEN))\n\nposition_b = np.zeros((len(IDS),MAX_LEN))\nposition_i = np.zeros((len(IDS),MAX_LEN))\n\nevidence_b = np.zeros((len(IDS),MAX_LEN))\nevidence_i = np.zeros((len(IDS),MAX_LEN))\n\nclaim_b = np.zeros((len(IDS),MAX_LEN))\nclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nconclusion_b = np.zeros((len(IDS),MAX_LEN))\nconclusion_i = np.zeros((len(IDS),MAX_LEN))\n\ncounterclaim_b = np.zeros((len(IDS),MAX_LEN))\ncounterclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nrebuttal_b = np.zeros((len(IDS),MAX_LEN))\nrebuttal_i = np.zeros((len(IDS),MAX_LEN))\n\n# HELPER VARIABLES\ntrain_lens = []\ntargets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\ntargets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\ntarget_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n             'Counterclaim':5, 'Rebuttal':6}","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:17.730579Z","iopub.status.busy":"2021-12-27T16:21:17.729658Z","iopub.status.idle":"2021-12-27T16:21:17.888933Z","shell.execute_reply":"2021-12-27T16:21:17.890092Z","shell.execute_reply.started":"2021-12-27T15:41:44.979282Z"},"papermill":{"duration":0.216801,"end_time":"2021-12-27T16:21:17.890321","exception":false,"start_time":"2021-12-27T16:21:17.673520","status":"completed"},"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Assertion: The code snippet starts with an assertion that checks whether the 'discourse_start' values within groups of the 'id' column are in ascending order. The assertion ensures that the text data is correctly ordered.\n\nLoop Through Train Texts: The code then enters a loop that iterates through each training text. It seems that the variable IDS should be defined somewhere before this code. It appears to represent a list of IDs related to the training texts.\n\nTokenization and Token Arrays: Inside the loop, the script reads the content of each training text from a file and tokenizes it using a tokenizer. The tokenized data is stored in arrays named train_tokens and train_attention, which are indexed by the id_num.\n\nFinding Targets in Text: The code then proceeds to find targets within the tokenized text. It appears that the targets are defined based on character offsets within the original text. The script matches these offsets with tokenized offsets to determine which tokens correspond to the target regions.\n\nTarget Arrays: The targets are saved in arrays, targets_b and targets_i, with appropriate indexing. The targets_b array represents the beginning of a target entity, while the targets_i array represents the continuation of a target entity.","metadata":{}},{"cell_type":"code","source":"# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\nassert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n\n# FOR LOOP THROUGH EACH TRAIN TEXT\nfor id_num in range(len(IDS)):\n    if LOAD_TOKENS_FROM: break\n    if id_num%100==0: print(id_num,', ',end='')\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = IDS[id_num]\n    name = f'../input/feedback-prize-2021/train/{n}.txt'\n    txt = open(name, 'r').read()\n    train_lens.append( len(txt.split()))\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    train_tokens[id_num,] = tokens['input_ids']\n    train_attention[id_num,] = tokens['attention_mask']\n    \n    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n    offsets = tokens['offset_mapping']\n    offset_index = 0\n    df = train.loc[train.id==n]\n    for index,row in df.iterrows():\n        a = row.discourse_start\n        b = row.discourse_end\n        if offset_index>len(offsets)-1:\n            break\n        c = offsets[offset_index][0]\n        d = offsets[offset_index][1]\n        beginning = True\n        while b>c:\n            if (c>=a)&(b>=d):\n                k = target_map[row.discourse_type]\n                if beginning:\n                    targets_b[k][id_num][offset_index] = 1\n                    beginning = False\n                else:\n                    targets_i[k][id_num][offset_index] = 1\n            offset_index += 1\n            if offset_index>len(offsets)-1:\n                break\n            c = offsets[offset_index][0]\n            d = offsets[offset_index][1]","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:17.998565Z","iopub.status.busy":"2021-12-27T16:21:17.997648Z","iopub.status.idle":"2021-12-27T16:21:20.571527Z","shell.execute_reply":"2021-12-27T16:21:20.570987Z","shell.execute_reply.started":"2021-12-27T15:41:45.134943Z"},"papermill":{"duration":2.634174,"end_time":"2021-12-27T16:21:20.571670","exception":false,"start_time":"2021-12-27T16:21:17.937496","status":"completed"},"tags":[]},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    plt.hist(train_lens,bins=100)\n    plt.title('Histogram of Train Word Counts',size=16)\n    plt.xlabel('Train Word Count',size=14)\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:20.639522Z","iopub.status.busy":"2021-12-27T16:21:20.638666Z","iopub.status.idle":"2021-12-27T16:21:20.640530Z","shell.execute_reply":"2021-12-27T16:21:20.640997Z","shell.execute_reply.started":"2021-12-27T15:41:47.587163Z"},"papermill":{"duration":0.03875,"end_time":"2021-12-27T16:21:20.641131","exception":false,"start_time":"2021-12-27T16:21:20.602381","status":"completed"},"tags":[]},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"\n\nFrom the histogram of train word counts above, we see that using a transformer width of 1024 is a good comprise of capturing most of the data's signal but not having too large a model. (Note that analyzing a histogram of train **token** counts would be better but we don't do that here). We could probably explore other widths between 512 and 1024 also. Or we could use widths of size 512 or smaller and use a stride which breaks a single text into multiple chunks (with possible overlap).","metadata":{"papermill":{"duration":0.029476,"end_time":"2021-12-27T16:21:20.701327","exception":false,"start_time":"2021-12-27T16:21:20.671851","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Creating Targets Array: A NumPy array called targets is created. It has a shape of (len(IDS), MAX_LEN, 15), where:\n\nlen(IDS) represents the number of training examples.\nMAX_LEN represents the maximum length of tokenized sequences.\n15 corresponds to the number of classes (twice the number of entity types plus one).\nPopulating Target Classes: Within a loop, the code iterates through the entity classes (seems like there are 7 classes). For each class:\n\nIt assigns the values from the targets_b array for the specific class to the even-numbered columns of the targets array.\nIt assigns the values from the targets_i array for the specific class to the odd-numbered columns of the targets array.\nAdditional Column for \"Not a Target\": An additional column is added to the targets array. This column (targets[:,:,14]) is filled with values that are derived by subtracting the maximum value of the targets array along the last axis from 1. This column seems to represent the absence of any target entity, possibly as a default or background class.","metadata":{}},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n    for k in range(7):\n        targets[:,:,2*k] = targets_b[k]\n        targets[:,:,2*k+1] = targets_i[k]\n    targets[:,:,14] = 1-np.max(targets,axis=-1)","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:20.766590Z","iopub.status.busy":"2021-12-27T16:21:20.766096Z","iopub.status.idle":"2021-12-27T16:21:20.769480Z","shell.execute_reply":"2021-12-27T16:21:20.769046Z","shell.execute_reply.started":"2021-12-27T15:41:47.595882Z"},"papermill":{"duration":0.038277,"end_time":"2021-12-27T16:21:20.769585","exception":false,"start_time":"2021-12-27T16:21:20.731308","status":"completed"},"tags":[]},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    np.save(f'targets_{MAX_LEN}', targets)\n    np.save(f'tokens_{MAX_LEN}', train_tokens)\n    np.save(f'attention_{MAX_LEN}', train_attention)\n    print('Saved NER tokens')\nelse:\n    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n    print('Loaded NER tokens')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:20.833148Z","iopub.status.busy":"2021-12-27T16:21:20.832223Z","iopub.status.idle":"2021-12-27T16:21:28.268572Z","shell.execute_reply":"2021-12-27T16:21:28.267755Z","shell.execute_reply.started":"2021-12-27T15:41:47.606180Z"},"papermill":{"duration":7.470377,"end_time":"2021-12-27T16:21:28.268764","exception":false,"start_time":"2021-12-27T16:21:20.798387","status":"completed"},"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Loaded NER tokens\n"}]},{"cell_type":"markdown","source":"# Build Model\nWe will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax. We use 15 classes because we have a `B` class and `I` class for each of 7 labels. And we have an additional class (called `O` class) for tokens that do not belong to one of the 14 classes.","metadata":{"papermill":{"duration":0.056273,"end_time":"2021-12-27T16:21:28.378021","exception":false,"start_time":"2021-12-27T16:21:28.321748","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**MODEL:**\n\nInput Layers: Two input layers are defined using tf.keras.layers.Input():\n\ntokens: Represents the tokenized input sequences, each of length MAX_LEN.\nattention: Represents the attention mask for the input sequences, also of length MAX_LEN.\nConfig and Backbone: The code uses the AutoConfig and TFAutoModel classes from the Hugging Face Transformers library to load the configuration and pre-trained Longformer model from the specified DOWNLOADED_MODEL_PATH.\n\nBackbone Architecture: The loaded pre-trained Longformer model is used as the backbone of the new model. The input tokens and attention masks are passed through the backbone model to obtain its output.\n\nDense Layers: Following the backbone, the output is passed through two dense layers:\n\nA dense layer with 256 units and ReLU activation function.\nA dense layer with 15 units and softmax activation function. This seems to be the final output layer that predicts probabilities for the 15 classes.\nModel Compilation: The final model is constructed using tf.keras.Model() with the defined inputs and outputs. It's then compiled using the Adam optimizer with a learning rate of 1e-4. The loss function used is categorical cross-entropy, which is appropriate for multiclass classification tasks. The accuracy metric is also defined as a performance measure.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'/tf_model.h5', config=config)\n    \n    x = backbone(tokens, attention_mask=attention)\n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n    \n    return model","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:28.482220Z","iopub.status.busy":"2021-12-27T16:21:28.481586Z","iopub.status.idle":"2021-12-27T16:21:28.485605Z","shell.execute_reply":"2021-12-27T16:21:28.485155Z","shell.execute_reply.started":"2021-12-27T15:41:55.764949Z"},"papermill":{"duration":0.050753,"end_time":"2021-12-27T16:21:28.485729","exception":false,"start_time":"2021-12-27T16:21:28.434976","status":"completed"},"tags":[]},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:21:28.552374Z","iopub.status.busy":"2021-12-27T16:21:28.551431Z","iopub.status.idle":"2021-12-27T16:21:59.973396Z","shell.execute_reply":"2021-12-27T16:21:59.973863Z","shell.execute_reply.started":"2021-12-27T15:41:55.776705Z"},"papermill":{"duration":31.456803,"end_time":"2021-12-27T16:21:59.974017","exception":false,"start_time":"2021-12-27T16:21:28.517214","status":"completed"},"tags":[]},"execution_count":15,"outputs":[{"name":"stderr","output_type":"stream","text":"All model checkpoint layers were used when initializing TFLongformerModel.\n\n\n\nAll the layers of TFLongformerModel were initialized from the model checkpoint at ../input/tf-longformer-v12/tf_model.h5.\n\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n\n/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"}]},{"cell_type":"markdown","source":"# Train or Load Model","metadata":{"papermill":{"duration":0.029782,"end_time":"2021-12-27T16:22:00.034161","exception":false,"start_time":"2021-12-27T16:22:00.004379","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 5\nBATCH_SIZE = 4 \nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5] \ndef lrfn(epoch):\n    return LRS[epoch]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:22:00.099242Z","iopub.status.busy":"2021-12-27T16:22:00.098498Z","iopub.status.idle":"2021-12-27T16:22:00.100595Z","shell.execute_reply":"2021-12-27T16:22:00.101005Z","shell.execute_reply.started":"2021-12-27T15:42:28.327499Z"},"papermill":{"duration":0.037039,"end_time":"2021-12-27T16:22:00.101140","exception":false,"start_time":"2021-12-27T16:22:00.064101","status":"completed"},"tags":[]},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:22:00.167846Z","iopub.status.busy":"2021-12-27T16:22:00.167076Z","iopub.status.idle":"2021-12-27T16:22:00.174093Z","shell.execute_reply":"2021-12-27T16:22:00.174526Z","shell.execute_reply.started":"2021-12-27T15:42:28.336159Z"},"papermill":{"duration":0.04328,"end_time":"2021-12-27T16:22:00.174640","exception":false,"start_time":"2021-12-27T16:22:00.131360","status":"completed"},"tags":[]},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"Train size 14034 , Valid size 1560\n"}]},{"cell_type":"code","source":"# LOAD MODEL\nif LOAD_MODEL_FROM:\n    model.load_weights(f'{LOAD_MODEL_FROM}/long_v{VER}.h5')\n    \n# OR TRAIN MODEL\nelse:\n    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n          y = targets[train_idx,],\n          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n                             targets[valid_idx,]),\n          callbacks = [lr_callback],\n          epochs = EPOCHS,\n          batch_size = BATCH_SIZE,\n          verbose = 2)\n\n    # SAVE MODEL WEIGHTS\n    model.save_weights(f'long_v{VER}.h5')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:22:00.249200Z","iopub.status.busy":"2021-12-27T16:22:00.248384Z","iopub.status.idle":"2021-12-27T16:22:06.838632Z","shell.execute_reply":"2021-12-27T16:22:06.838076Z","shell.execute_reply.started":"2021-12-27T15:42:28.352537Z"},"papermill":{"duration":6.633649,"end_time":"2021-12-27T16:22:06.838781","exception":false,"start_time":"2021-12-27T16:22:00.205132","status":"completed"},"scrolled":true,"tags":[]},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Validate Model - Infer OOF\nWe will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices.","metadata":{"papermill":{"duration":0.030811,"end_time":"2021-12-27T16:22:06.901147","exception":false,"start_time":"2021-12-27T16:22:06.870336","status":"completed"},"tags":[]}},{"cell_type":"code","source":"p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n                  batch_size=16, verbose=2)\nprint('OOF predictions shape:',p.shape)\noof_preds = np.argmax(p,axis=-1)","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:22:06.968775Z","iopub.status.busy":"2021-12-27T16:22:06.967729Z","iopub.status.idle":"2021-12-27T16:25:44.555333Z","shell.execute_reply":"2021-12-27T16:25:44.555856Z","shell.execute_reply.started":"2021-12-27T15:42:36.315501Z"},"papermill":{"duration":217.623905,"end_time":"2021-12-27T16:25:44.556025","exception":false,"start_time":"2021-12-27T16:22:06.932120","status":"completed"},"tags":[]},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"98/98 - 186s\n\nOOF predictions shape: (1560, 1024, 15)\n"}]},{"cell_type":"code","source":"target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:25:44.623151Z","iopub.status.busy":"2021-12-27T16:25:44.622354Z","iopub.status.idle":"2021-12-27T16:25:44.624251Z","shell.execute_reply":"2021-12-27T16:25:44.624739Z","shell.execute_reply.started":"2021-12-27T15:45:44.043517Z"},"papermill":{"duration":0.037789,"end_time":"2021-12-27T16:25:44.624893","exception":false,"start_time":"2021-12-27T16:25:44.587104","status":"completed"},"tags":[]},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n    all_predictions = []\n\n    for id_num in range(len(preds)):\n    \n        # GET ID\n        if (id_num%100==0)&(verbose): \n            print(id_num,', ',end='')\n        n = text_ids[id_num]\n    \n        # GET TOKEN POSITIONS IN CHARS\n        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\n        txt = open(name, 'r').read()\n        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n        off = tokens['offset_mapping']\n    \n        # GET WORD POSITIONS IN CHARS\n        w = []\n        blank = True\n        for i in range(len(txt)):\n            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n                w.append(i)\n                blank=False\n            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n                blank=True\n        w.append(1e6)\n            \n        # MAPPING FROM TOKENS TO WORDS\n        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1]==0: continue\n            while off[i][0]>=w[w_i+1]: w_i += 1\n            word_map[i] = int(w_i)\n        \n        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n        ### KEY: ###\n        # 0: LEAD_B, 1: LEAD_I\n        # 2: POSITION_B, 3: POSITION_I\n        # 4: EVIDENCE_B, 5: EVIDENCE_I\n        # 6: CLAIM_B, 7: CLAIM_I\n        # 8: CONCLUSION_B, 9: CONCLUSION_I\n        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n        # 12: REBUTTAL_B, 13: REBUTTAL_I\n        # 14: NOTHING i.e. O\n        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n        pred = preds[id_num,]/2.0\n    \n        i = 0\n        while i<MAX_LEN:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]:\n                prediction.append(word_map[i])\n                i += 1\n                if i>=MAX_LEN: break\n                while pred[i]==start+0.5:\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i>=MAX_LEN: break\n            else:\n                i += 1\n            prediction = [x for x in prediction if x!=-1]\n            if len(prediction)>4:\n                all_predictions.append( (n, target_map_rev[int(start)], \n                                ' '.join([str(x) for x in prediction]) ) )\n                \n    # MAKE DATAFRAME\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id','class','predictionstring']\n    \n    return df","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:25:44.702957Z","iopub.status.busy":"2021-12-27T16:25:44.702136Z","iopub.status.idle":"2021-12-27T16:25:44.704559Z","shell.execute_reply":"2021-12-27T16:25:44.704147Z","shell.execute_reply.started":"2021-12-27T15:45:44.050699Z"},"papermill":{"duration":0.048639,"end_time":"2021-12-27T16:25:44.704662","exception":false,"start_time":"2021-12-27T16:25:44.656023","status":"completed"},"tags":[]},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\noof.head()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:25:44.770090Z","iopub.status.busy":"2021-12-27T16:25:44.769552Z","iopub.status.idle":"2021-12-27T16:26:06.867939Z","shell.execute_reply":"2021-12-27T16:26:06.867311Z","shell.execute_reply.started":"2021-12-27T15:45:44.076202Z"},"papermill":{"duration":22.132506,"end_time":"2021-12-27T16:26:06.868079","exception":false,"start_time":"2021-12-27T16:25:44.735573","status":"completed"},"tags":[]},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , "},{"execution_count":22,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>class</th>\n","      <th>predictionstring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>50B3435E475B</td>\n","      <td>Lead</td>\n","      <td>3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50B3435E475B</td>\n","      <td>Position</td>\n","      <td>63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50B3435E475B</td>\n","      <td>Claim</td>\n","      <td>76 77 78 79 80 81 82 83 84 85 86 87</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>50B3435E475B</td>\n","      <td>Evidence</td>\n","      <td>88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>50B3435E475B</td>\n","      <td>Claim</td>\n","      <td>162 163 164 165 166 167 168 169 170 171 172 17...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id     class                                   predictionstring\n","0  50B3435E475B      Lead  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...\n","1  50B3435E475B  Position             63 64 65 66 67 68 69 70 71 72 73 74 75\n","2  50B3435E475B     Claim                76 77 78 79 80 81 82 83 84 85 86 87\n","3  50B3435E475B  Evidence  88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...\n","4  50B3435E475B     Claim  162 163 164 165 166 167 168 169 170 171 172 17..."]},"metadata":{}}]},{"cell_type":"code","source":"print('The following classes are present in oof preds:')\noof['class'].unique()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:06.950714Z","iopub.status.busy":"2021-12-27T16:26:06.950167Z","iopub.status.idle":"2021-12-27T16:26:06.957766Z","shell.execute_reply":"2021-12-27T16:26:06.958200Z","shell.execute_reply.started":"2021-12-27T15:46:10.028346Z"},"papermill":{"duration":0.050625,"end_time":"2021-12-27T16:26:06.958336","exception":false,"start_time":"2021-12-27T16:26:06.907711","status":"completed"},"tags":[]},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"The following classes are present in oof preds:\n"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":["array(['Lead', 'Position', 'Claim', 'Evidence', 'Concluding Statement',\n","       'Counterclaim', 'Rebuttal'], dtype=object)"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Compute Validation Metric\nThe following code is from Rob Mulla's excellent notebook [here][2]. Our LongFormer single fold model achieves CV score 0.633! Hooray!\n\n[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch","metadata":{"papermill":{"duration":0.03651,"end_time":"2021-12-27T16:26:07.033076","exception":false,"start_time":"2021-12-27T16:26:06.996566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# CODE FROM : Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:07.107543Z","iopub.status.busy":"2021-12-27T16:26:07.106692Z","iopub.status.idle":"2021-12-27T16:26:07.120912Z","shell.execute_reply":"2021-12-27T16:26:07.120474Z","shell.execute_reply.started":"2021-12-27T15:46:10.040792Z"},"papermill":{"duration":0.052066,"end_time":"2021-12-27T16:26:07.121023","exception":false,"start_time":"2021-12-27T16:26:07.068957","status":"completed"},"tags":[]},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# VALID DATAFRAME\nvalid = train.loc[train['id'].isin(IDS[valid_idx])]","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:07.214510Z","iopub.status.busy":"2021-12-27T16:26:07.213843Z","iopub.status.idle":"2021-12-27T16:26:07.221115Z","shell.execute_reply":"2021-12-27T16:26:07.220689Z","shell.execute_reply.started":"2021-12-27T15:46:10.066554Z"},"papermill":{"duration":0.064177,"end_time":"2021-12-27T16:26:07.221231","exception":false,"start_time":"2021-12-27T16:26:07.157054","status":"completed"},"tags":[]},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"f1s = []\nCLASSES = oof['class'].unique()\nfor c in CLASSES:\n    pred_df = oof.loc[oof['class']==c].copy()\n    gt_df = valid.loc[valid['discourse_type']==c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c,f1)\n    f1s.append(f1)\nprint()\nprint('Overall',np.mean(f1s))","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:07.298468Z","iopub.status.busy":"2021-12-27T16:26:07.297746Z","iopub.status.idle":"2021-12-27T16:26:09.458694Z","shell.execute_reply":"2021-12-27T16:26:09.458144Z","shell.execute_reply.started":"2021-12-27T15:46:10.106365Z"},"papermill":{"duration":2.201911,"end_time":"2021-12-27T16:26:09.458895","exception":false,"start_time":"2021-12-27T16:26:07.256984","status":"completed"},"tags":[]},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"Lead 0.8063284233496999\n\nPosition 0.6841560234725578\n\nClaim 0.6057328285559762\n\nEvidence 0.6816788493279887\n\nConcluding Statement 0.7827050997782705\n\nCounterclaim 0.4854732895970009\n\nRebuttal 0.39030955585464333\n\n\n\nOverall 0.6337691528480197\n"}]},{"cell_type":"markdown","source":"# Infer Test Data\nWe will now infer the test data and create a submission. Our CV is 0.633, let's see what our LB is!","metadata":{"papermill":{"duration":0.037466,"end_time":"2021-12-27T16:26:09.534850","exception":false,"start_time":"2021-12-27T16:26:09.497384","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# GET TEST TEXT IDS\nfiles = os.listdir('../input/feedback-prize-2021/test')\nTEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\nprint('There are',len(TEST_IDS),'test texts.')","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:09.613643Z","iopub.status.busy":"2021-12-27T16:26:09.613132Z","iopub.status.idle":"2021-12-27T16:26:09.618923Z","shell.execute_reply":"2021-12-27T16:26:09.619329Z","shell.execute_reply.started":"2021-12-27T15:46:12.640038Z"},"papermill":{"duration":0.047293,"end_time":"2021-12-27T16:26:09.619455","exception":false,"start_time":"2021-12-27T16:26:09.572162","status":"completed"},"tags":[]},"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":"There are 5 test texts.\n"}]},{"cell_type":"code","source":"# CONVERT TEST TEXT TO TOKENS\ntest_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\ntest_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n\nfor id_num in range(len(TEST_IDS)):\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = TEST_IDS[id_num]\n    name = f'../input/feedback-prize-2021/test/{n}.txt'\n    txt = open(name, 'r').read()\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    test_tokens[id_num,] = tokens['input_ids']\n    test_attention[id_num,] = tokens['attention_mask']","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:09.701146Z","iopub.status.busy":"2021-12-27T16:26:09.700545Z","iopub.status.idle":"2021-12-27T16:26:09.735834Z","shell.execute_reply":"2021-12-27T16:26:09.735298Z","shell.execute_reply.started":"2021-12-27T15:46:12.657057Z"},"papermill":{"duration":0.078442,"end_time":"2021-12-27T16:26:09.735936","exception":false,"start_time":"2021-12-27T16:26:09.657494","status":"completed"},"tags":[]},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# INFER TEST TEXTS\np = model.predict([test_tokens, test_attention], \n                  batch_size=16, verbose=2)\nprint('Test predictions shape:',p.shape)\ntest_preds = np.argmax(p,axis=-1)","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:09.816483Z","iopub.status.busy":"2021-12-27T16:26:09.815767Z","iopub.status.idle":"2021-12-27T16:26:10.451751Z","shell.execute_reply":"2021-12-27T16:26:10.451348Z","shell.execute_reply.started":"2021-12-27T15:46:12.724253Z"},"papermill":{"duration":0.678052,"end_time":"2021-12-27T16:26:10.451929","exception":false,"start_time":"2021-12-27T16:26:09.773877","status":"completed"},"tags":[]},"execution_count":29,"outputs":[{"name":"stdout","output_type":"stream","text":"1/1 - 1s\n\nTest predictions shape: (5, 1024, 15)\n"}]},{"cell_type":"markdown","source":"# Write Submission CSV","metadata":{"papermill":{"duration":0.038849,"end_time":"2021-12-27T16:26:10.529192","exception":false,"start_time":"2021-12-27T16:26:10.490343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# GET TEST PREDICIONS\nsub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\nsub.head()","metadata":{"execution":{"iopub.execute_input":"2021-12-27T16:26:10.614264Z","iopub.status.busy":"2021-12-27T16:26:10.613533Z","iopub.status.idle":"2021-12-27T16:26:10.692727Z","shell.execute_reply":"2021-12-27T16:26:10.692321Z","shell.execute_reply.started":"2021-12-27T15:46:13.383247Z"},"papermill":{"duration":0.125038,"end_time":"2021-12-27T16:26:10.692866","exception":false,"start_time":"2021-12-27T16:26:10.567828","status":"completed"},"tags":[]},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>class</th>\n","      <th>predictionstring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Lead</td>\n","      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Position</td>\n","      <td>41 42 43 44 45 46 47</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Claim</td>\n","      <td>66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Evidence</td>\n","      <td>120 121 122 123 124 125 126 127 128 129 130 13...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Claim</td>\n","      <td>196 197 198 199 200 201 202 203 204 205 206 20...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id     class                                   predictionstring\n","0  0FB0700DAF44      Lead  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n","1  0FB0700DAF44  Position                               41 42 43 44 45 46 47\n","2  0FB0700DAF44     Claim  66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...\n","3  0FB0700DAF44  Evidence  120 121 122 123 124 125 126 127 128 129 130 13...\n","4  0FB0700DAF44     Claim  196 197 198 199 200 201 202 203 204 205 206 20..."]},"metadata":{}}]}]}